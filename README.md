# Word_Level_Sign_lang_Recognition
Keywords: Effective Communication, NLP, Sign Language Recognition, CNN, LSTM, ASL

Effective communication is crucial for individuals with hearing impairments, where sign language acts as a vital bridge[1]. Traditional methods[2], such as human translators, often fall short, leading to delays and inconsistencies. AI-based solutions, particularly those driven by Natural Language Processing (NLP), offer a promising alternative by accurately translating sign language gestures into text. Our project focuses on refining AI systems to enhance the interpretation of American Sign Language (ASL) gestures into English words, addressing critical research gaps in model accuracy. We integrate algorithms, Convolutional Neural Networks (CNNs), and Long Short-Term Memory (LSTM) networks, for comprehensive gesture recognition and translation. This endeavor aims to create a model capable of recognizing signs corresponding to English words with better accuracy, laying the groundwork for future sentence-level Sign Language Recognition (SLR) solutions. 
